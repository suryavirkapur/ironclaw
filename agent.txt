current context:
- step 1 host-only llm flow is implemented in ironclawd
- ironclawd websocket user messages now call openai from host using reqwest and return streamdelta json to ui
- auth challenge/auth ack handshake and cap token enforcement remain active on host-guest transport
- irowclaw is still running for vm/session plumbing but no host websocket user message is forwarded to guest llm path
- host config now includes llm model, base_url, and api selector fields in toml

pending tasks:
- step 2 tool call loop: let host llm produce tool requests and execute via existing host tool policy
- add stronger integration test coverage for websocket + llm path with mocked http service
- validate firecracker e2e behavior with real openai key in a controlled smoke run

important decisions:
- llm network calls are host-only and keyed by openai_api_key env var
- guest must never call llm endpoints
- default host llm api is responses at https://api.openai.com/v1 with model gpt-5-mini

architectural notes:
- websocket payload stays messageenvelope json with payload.streamdelta for text output
- host keeps vm start and authenticated transport setup intact before ws bridge loop
- host tool request handling from guest remains wired but tool use is not initiated in step 1

plan:
- keep step 1 stable and verified with cargo check and smoke-firecracker
- implement step 2 host tool orchestration through llm output schema

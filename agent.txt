current context:
- memory crate now implements hybrid retrieval with sqlite fts5 + blob vectors and weighted fusion
- vector search uses pure-sql cosine ranking with sqlite scalar functions (`vec_dot`, `vec_norm`)
- markdown indexing now preserves heading hierarchy and stable chunk ids for document provenance
- guest runtime retrieval now uses hybrid search config and returns path, heading, ordinal, and text

pending tasks:
- validate retrieval quality against real project markdown corpus and tune fusion weights if needed
- decide whether to add remote embedding provider support beyond local `simple-384` fallback
- run full smoke tests in an environment with network and vm permissions before release

important decisions:
- kept sqlite-only storage for vectors and cache; no external vector database dependency
- added backward-compatible config aliases for `semantic_weight` and `lexical_weight`
- used lru cache policy by updating `embedding_cache.created_at` on hit and pruning oldest rows

architectural notes:
- indexing flow: markdown chunk -> chunk upsert/delete -> atomic fts rebuild
  -> re-embed missing vectors
- retrieval flow: bm25 fts + semantic cosine -> weighted fusion -> provenance summary output
- fallback flow: if embedding generation fails, retrieval falls back to sql like matching

plan:
- monitor retrieval relevance and cache hit rate during real usage
- add optional telemetry counters for indexing/retrieval latency if performance tuning is needed

current context:
- step 2 host-only tool planning/execution loop is implemented in ironclawd
- ironclawd websocket user messages now use host llm planning json (`tool` or `answer`)
- when planned, host executes allowlisted tools directly and sends truncated tool output
  into host llm for final response
- auth challenge/auth ack handshake and cap token enforcement remain active on host-guest transport
- irowclaw is still running for vm/session plumbing and can still request host
  tool execution via authenticated transport
- host config now includes llm model, base_url, and api selector fields in toml

pending tasks:
- add stronger integration test coverage for websocket + llm path with mocked http service
- validate firecracker e2e behavior with real openai key in a controlled smoke run

important decisions:
- llm network calls are host-only and keyed by openai_api_key env var
- guest must never call llm endpoints
- default host llm api is responses at https://api.openai.com/v1 with model gpt-5-mini

architectural notes:
- websocket payload stays messageenvelope json with payload.streamdelta for text output
- host keeps vm start and authenticated transport setup intact before ws bridge loop
- host tool request handling from guest remains wired but tool use is not initiated in step 1

plan:
- keep step 2 stable and verified with cargo check and smoke-firecracker
- continue with firecracker guest transport and host-guest e2e bridge hardening
